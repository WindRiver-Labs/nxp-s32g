From 905f4430154e0be9b233ac33930d55939c2b91a2 Mon Sep 17 00:00:00 2001
From: Bogdan Folea <bogdan.folea@nxp.com>
Date: Fri, 12 Nov 2021 10:07:52 +0200
Subject: [PATCH 21/24] crypto: hse: fix hwrng to handle non-blocking read

commit 905f4430154e0be9b233ac33930d55939c2b91a2 from
https://source.codeaurora.org/external/autobsps32/linux

Issue: ALB-4038
Upstream-Status: Pending 

Signed-off-by: Bogdan Folea <bogdan.folea@nxp.com>
Signed-off-by: Zhantao Tang <zhantao.tang@windriver.com>
---
 drivers/crypto/hse/hse-core.c |   6 +-
 drivers/crypto/hse/hse-core.h |   4 +-
 drivers/crypto/hse/hse-rng.c  | 177 +++++++++++++++++++++++++++-------
 3 files changed, 147 insertions(+), 40 deletions(-)

diff --git a/drivers/crypto/hse/hse-core.c b/drivers/crypto/hse/hse-core.c
index dc674e13a7ab..5b4b6aa80a56 100644
--- a/drivers/crypto/hse/hse-core.c
+++ b/drivers/crypto/hse/hse-core.c
@@ -799,7 +799,7 @@ static irqreturn_t hse_evt_dispatcher(int irq, void *dev)
 	if (IS_ENABLED(CONFIG_CRYPTO_DEV_NXP_HSE_AEAD))
 		hse_aead_unregister(&drv->aead_algs);
 	if (IS_ENABLED(CONFIG_CRYPTO_DEV_NXP_HSE_HWRNG))
-		hse_hwrng_unregister(dev);
+		hse_rng_unregister(dev);
 
 	dev_crit(dev, "communication terminated, reset system to recover\n");
 
@@ -900,7 +900,7 @@ static int hse_probe(struct platform_device *pdev)
 	if (IS_ENABLED(CONFIG_CRYPTO_DEV_NXP_HSE_AEAD))
 		hse_aead_register(dev, &drv->aead_algs);
 	if (IS_ENABLED(CONFIG_CRYPTO_DEV_NXP_HSE_HWRNG))
-		hse_hwrng_register(dev);
+		hse_rng_register(dev);
 
 	dev_info(dev, "device ready, status 0x%04X\n", status);
 
@@ -927,7 +927,7 @@ static int hse_remove(struct platform_device *pdev)
 	if (IS_ENABLED(CONFIG_CRYPTO_DEV_NXP_HSE_AEAD))
 		hse_aead_unregister(&drv->aead_algs);
 	if (IS_ENABLED(CONFIG_CRYPTO_DEV_NXP_HSE_HWRNG))
-		hse_hwrng_unregister(dev);
+		hse_rng_unregister(dev);
 
 	/* empty used key rings */
 	hse_key_ring_free(&drv->aes_key_ring);
diff --git a/drivers/crypto/hse/hse-core.h b/drivers/crypto/hse/hse-core.h
index a55a3192385e..ad0d50d75ddf 100644
--- a/drivers/crypto/hse/hse-core.h
+++ b/drivers/crypto/hse/hse-core.h
@@ -60,8 +60,8 @@ void hse_skcipher_unregister(struct list_head *alg_list);
 void hse_aead_register(struct device *dev, struct list_head *alg_list);
 void hse_aead_unregister(struct list_head *alg_list);
 
-void hse_hwrng_register(struct device *dev);
-void hse_hwrng_unregister(struct device *dev);
+void hse_rng_register(struct device *dev);
+void hse_rng_unregister(struct device *dev);
 
 u32 _get_rng_srv_id(struct device *dev);
 
diff --git a/drivers/crypto/hse/hse-rng.c b/drivers/crypto/hse/hse-rng.c
index b594d77df10c..e8ce89bbf38b 100644
--- a/drivers/crypto/hse/hse-rng.c
+++ b/drivers/crypto/hse/hse-rng.c
@@ -1,8 +1,8 @@
 // SPDX-License-Identifier: BSD 3-clause
 /*
- * NXP HSE Driver - Hardware True RNG Support
+ * NXP HSE Driver - Hardware RNG Support
  *
- * This file contains hw_random framework support for HSE hardware true RNG.
+ * This file contains the hw_random framework support for HSE hardware TRNG.
  *
  * Copyright 2019-2021 NXP
  */
@@ -16,72 +16,182 @@
 
 #define HSE_RNG_QUALITY    1024u /* number of entropy bits per 1024 bits */
 
+#define HSE_RNG_CACHE_MIN    64u /* minimum threshold for cache refill */
+#define HSE_RNG_CACHE_MAX    512u /* total size of driver internal cache */
+
 /**
- * struct hse_rng_ctx - hwrng context
- * @srv_desc: service descriptor
+ * struct hse_rng_ctx - RNG context
+ * @cache: driver internal random data cache
  * @dev: HSE device
- * @req_lock: service descriptor mutex
+ * @srv_desc: service descriptor used for cache refill
+ * @cache_idx: current index in internal cache
+ * @cache_dma: DMA address of internal cache
+ * @req_lock: mutex used for cache refill operations
  */
 struct hse_rng_ctx {
-	struct hse_srv_desc srv_desc;
+	u8 cache[HSE_RNG_CACHE_MAX];
 	struct device *dev;
-	struct mutex req_lock; /* descriptor mutex */
+	struct hse_srv_desc srv_desc;
+	size_t cache_idx;
+	dma_addr_t cache_dma;
+	struct mutex req_lock; /* cache request mutex */
 };
 
 /**
- * hse_hwrng_read - generate random bytes of data into a supplied buffer
+ * hse_rng_done - RNG request done callback
+ * @err: service response error code
+ * @_ctx: RNG context
+ */
+static void hse_rng_done(int err, void *_ctx)
+{
+	struct hse_rng_ctx *ctx = (struct hse_rng_ctx *)_ctx;
+
+	if (unlikely(err))
+		dev_dbg(ctx->dev, "%s: request failed: %d\n", __func__, err);
+	else
+		ctx->cache_idx += ctx->srv_desc.rng_req.random_num_len;
+
+	mutex_unlock(&ctx->req_lock);
+}
+
+/**
+ * hse_rng_refill_cache - refill internal cache if below threshold
+ * @rng: hwrng instance
+ */
+static void hse_rng_refill_cache(struct hwrng *rng)
+{
+	struct hse_rng_ctx *ctx = (struct hse_rng_ctx *)rng->priv;
+	int err;
+
+	if (ctx->cache_idx >= HSE_RNG_CACHE_MIN)
+		return;
+
+	if (!mutex_trylock(&ctx->req_lock)) {
+		dev_dbg(ctx->dev, "%s: other request in progress\n", __func__);
+		return;
+	}
+
+	ctx->cache_idx = 0; /* discard remining data bytes */
+	ctx->srv_desc.rng_req.random_num_len = HSE_RNG_CACHE_MAX;
+	ctx->srv_desc.rng_req.random_num = ctx->cache_dma;
+
+	err = hse_srv_req_async(ctx->dev, HSE_CHANNEL_ANY, &ctx->srv_desc, ctx,
+				hse_rng_done);
+	if (unlikely(err)) {
+		mutex_unlock(&ctx->req_lock);
+		dev_dbg(ctx->dev, "%s: request failed: %d\n", __func__, err);
+	}
+}
+
+/**
+ * hse_rng_read - generate random bytes of data into a supplied buffer
  * @rng: hwrng instance
  * @buf: destination buffer
  * @count: number of bytes, multiple of 4, more than 32 bytes, less than 2k
- * @wait: wait for data
+ * @wait: synchronous request flag, set by upper layer if it can wait for data
  *
- * HSE will always provide the exact number of bytes requested up to 2k or
- * will return zero in case of any error.
+ * If possible, get random data from internal cache and trigger a refill.
+ * Otherwise, if the upper layer can wait, send a synchronous request to HSE.
+ * HSE will always provide the exact number of bytes requested, up to 2k.
+ *
+ * Return: number of random bytes on success, -EINVAL for invalid parameter,
+ *         -ENOMEM for DMA mapping failed, -EIO for service request error
  */
-static int hse_hwrng_read(struct hwrng *rng, void *buf, size_t count, bool wait)
+static int hse_rng_read(struct hwrng *rng, void *buf, size_t count, bool wait)
 {
 	struct hse_rng_ctx *ctx = (struct hse_rng_ctx *)rng->priv;
-	dma_addr_t data_dma;
+	struct hse_srv_desc srv_desc;
+	dma_addr_t buf_dma;
 	int err;
 
-	if (!mutex_trylock(&ctx->req_lock)) {
-		dev_dbg(ctx->dev, "%s: request in progress\n", __func__);
-		return 0;
+	count = min(count, HSE_MAX_RNG_SIZE);
+
+	if (ctx->cache_idx > 0) {
+		count = min(count, ctx->cache_idx);
+
+		dma_sync_single_for_cpu(ctx->dev, ctx->cache_dma - count +
+					ctx->cache_idx, count, DMA_FROM_DEVICE);
+		memcpy(buf, &ctx->cache[ctx->cache_idx - count], count);
+		ctx->cache_idx -= count;
+
+		/* refill cache if depleted */
+		hse_rng_refill_cache(rng);
+
+		return count;
 	}
 
-	data_dma = dma_map_single(ctx->dev, buf, count, DMA_FROM_DEVICE);
-	if (unlikely(dma_mapping_error(ctx->dev, data_dma))) {
-		mutex_unlock(&ctx->req_lock);
+	if (!wait) {
+		hse_rng_refill_cache(rng);
 		return 0;
 	}
 
-	ctx->srv_desc.srv_id = _get_rng_srv_id(ctx->dev);
-	ctx->srv_desc.rng_req.rng_class = HSE_RNG_CLASS_PTG3;
-	ctx->srv_desc.rng_req.random_num_len = count;
-	ctx->srv_desc.rng_req.random_num = data_dma;
+	if (unlikely(count < HSE_MIN_RNG_SIZE))
+		return -EINVAL;
+
+	buf_dma = dma_map_single(ctx->dev, buf, count, DMA_FROM_DEVICE);
+	if (unlikely(dma_mapping_error(ctx->dev, buf_dma)))
+		return -ENOMEM;
+
+	srv_desc.srv_id = _get_rng_srv_id(ctx->dev);
+	srv_desc.rng_req.rng_class = HSE_RNG_CLASS_PTG3;
+	srv_desc.rng_req.random_num_len = count;
+	srv_desc.rng_req.random_num = buf_dma;
 
-	err = hse_srv_req_sync(ctx->dev, HSE_CHANNEL_ANY, &ctx->srv_desc);
+	err = hse_srv_req_sync(ctx->dev, HSE_CHANNEL_ANY, &srv_desc);
 	if (err)
 		dev_dbg(ctx->dev, "%s: request failed: %d\n", __func__, err);
 
-	dma_unmap_single(ctx->dev, data_dma, count, DMA_FROM_DEVICE);
+	dma_unmap_single(ctx->dev, buf_dma, count, DMA_FROM_DEVICE);
 
-	mutex_unlock(&ctx->req_lock);
+	return err ? -EIO : count;
+}
+
+/**
+ * hse_rng_init - initialize RNG
+ * @rng: hwrng instance
+ */
+static int hse_rng_init(struct hwrng *rng)
+{
+	struct hse_rng_ctx *ctx = (struct hse_rng_ctx *)rng->priv;
 
-	return !err ? count : 0;
+	mutex_init(&ctx->req_lock);
+
+	ctx->srv_desc.srv_id = _get_rng_srv_id(ctx->dev);
+	ctx->srv_desc.rng_req.rng_class = HSE_RNG_CLASS_PTG3;
+
+	ctx->cache_dma = dma_map_single(ctx->dev, ctx->cache, HSE_RNG_CACHE_MAX,
+					DMA_FROM_DEVICE);
+	if (unlikely(dma_mapping_error(ctx->dev, ctx->cache_dma)))
+		return -ENOMEM;
+
+	return 0;
+}
+
+/**
+ * hse_rng_cleanup - RNG cleanup
+ * @rng: hwrng instance
+ */
+static void hse_rng_cleanup(struct hwrng *rng)
+{
+	struct hse_rng_ctx *ctx = (struct hse_rng_ctx *)rng->priv;
+
+	dma_unmap_single(ctx->dev, ctx->cache_dma, HSE_RNG_CACHE_MAX,
+			 DMA_FROM_DEVICE);
 }
 
 static struct hwrng hse_rng = {
 	.name = "hwrng-hse",
-	.read = hse_hwrng_read,
+	.init = hse_rng_init,
+	.cleanup = hse_rng_cleanup,
+	.read = hse_rng_read,
 	.quality = HSE_RNG_QUALITY,
 };
 
 /**
- * hse_hwrng_register - register random number generator
+ * hse_rng_register - register RNG
  * @dev: HSE device
  */
-void hse_hwrng_register(struct device *dev)
+void hse_rng_register(struct device *dev)
 {
 	struct hse_rng_ctx *ctx;
 	int err;
@@ -89,11 +199,8 @@ void hse_hwrng_register(struct device *dev)
 	ctx = devm_kzalloc(dev, sizeof(*ctx), GFP_KERNEL);
 	if (IS_ERR_OR_NULL(ctx))
 		return;
-
 	ctx->dev = dev;
 
-	mutex_init(&ctx->req_lock);
-
 	hse_rng.priv = (unsigned long)ctx;
 
 	err = devm_hwrng_register(dev, &hse_rng);
@@ -106,10 +213,10 @@ void hse_hwrng_register(struct device *dev)
 }
 
 /**
- * hse_hwrng_unregister - unregister random number generator
+ * hse_rng_unregister - unregister RNG
  * @dev: HSE device
  */
-void hse_hwrng_unregister(struct device *dev)
+void hse_rng_unregister(struct device *dev)
 {
 	devm_hwrng_unregister(dev, &hse_rng);
 
-- 
2.17.1

